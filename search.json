[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "See how easy it is to translate text, get a transcription from a video, or generate beautiful images or meaningful text. Many things once thought impossible have moved from the too hard basket to the too easy basket, most just within the last two years.\nThis blog is for ML musings, explorations and learning, data science and notes to offset fading memory.\n‚ÄìAllen\n\nInferred.ai is a small guild of like-minded Data Scientists, Machine Learning Practitioners, Software Engineers and Problem Solvers.\nWhilst consulting pays the bills, we also volunteer our time and skills on projects for community groups, through ‚ÄúMachine Learning for Good‚Äù projects.\nWe are based in Brisbane, Australia.\nIf you have a project you wish us to consider, please send your enquiry to projects@inferred.ai"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Destinations Unknown",
    "section": "",
    "text": "Generating Kamon Designs\n\n\n\n\n\n\n\ngenerative\n\n\nfastai\n\n\n\n\nTraining a (DDPM) Denoising Diffusion Probabilistic Model on a Kamon dataset\n\n\n\n\n\n\nOct 17, 2022\n\n\nAllen Kamp\n\n\n\n\n\n\n  \n\n\n\n\nData Science and Machine Learning Resources\n\n\n\n\n\n\n\ndata science\n\n\n\n\nA collection of useful links to resources and tools.\n\n\n\n\n\n\nSep 25, 2022\n\n\nAllen Kamp\n\n\n\n\n\n\n  \n\n\n\n\nManaging Search - How to remove pages from search engines\n\n\n\n\n\n\n\nSEO\n\n\n\n\nHow to influence search engine indexing through Google Search Console and Meta-Tags.\n\n\n\n\n\n\nNov 15, 2021\n\n\nAllen Kamp\n\n\n\n\n\n\n  \n\n\n\n\nSnippets, Notes and Reminders.\n\n\n\n\n\n\n\nnotes\n\n\n\n\nA collection of snippets for fastai, github and jupyter notebooks\n\n\n\n\n\n\nNov 8, 2021\n\n\nAllen Kamp\n\n\n\n\n\n\n  \n\n\n\n\nIs the machine learning?\n\n\n\n\n\n\n\ncomputing history\n\n\n\n\nDo you remember the moment you knew that something fundamental had happened? The moment when a door opened to your future and there was no way you were ever going back.\n\n\n\n\n\n\nOct 25, 2021\n\n\nAllen Kamp\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2021-10-21-resources.html",
    "href": "posts/2021-10-21-resources.html",
    "title": "Data Science and Machine Learning Resources",
    "section": "",
    "text": "Free Online Classes & Learning Resources\nOne of the great things about the fields of data science and machine learning is the number of free courses from industry leaders.\n\nfastai\n\nPractical Deep Learning for Coders - A beginner friendly video lesson series from fastai using Python, Pytorch. Jeremy Howard & Rachel Thomas. The only prerequisite is that you know how to code (a year of experience is enough), preferably in Python, and that you have at least followed a high school math course. No special hardware needed as lessons can be run in Jupyter Notebooks on Google Colab. The accompanying book: Deep Learning for Coders with Fastai and Pytorch: AI Applications Without a PhD (Howard, J. and Gugger, S.)\nFastbook Reading Group Indepth tuturial video lessons to go along with the FastAI book. Highly recommended to augment your learning as your follow the fastai course & book (above).\nA Walk With Fastai Zac Mueller walks through Deep Learning examples & tips using the fastai library and Pytorch. Lessons include Vision, Tabular & Audio.\nAwesome fastai - A curated list of awesome projects and resources related fastai. Thanks to Tanishq Abraham\nFast AI Recommended Learning Resources for Python - A curated list of Python learning resources from Beginner to Advanced. Something for everyone. if you aren‚Äôt already a member of the fastai forums, now would be a great time to sign up, it is a welcoming, positive and helpful ML community.\nNYU Deep Learning SP21 - Video Course with accompanying website with lectures from Neural Net pioneer Yann LeCun and tutorials by Alfredo Canziani.\nComputational Thinking MIT Spring 2021 - A computational thinking course in Julia language. Julia is the up and coming language for DS and ML that may well displace Python & R in the future. Also see Introduction to Computational Thinking\nProgramming of Simulation, Analysis, and Learning Systems - UQ - A practical way to learn Julia through simulation and analysis. From bubble sort to Monte Carlo.\nKhan Academy Learn and refresh skills across Math, Statistics and more.\n3Blue1Brown Math concepts explained in video with beautiful animations and visualisations. You can spend many enjoyable hours here even if you don‚Äôt think you are a ‚ÄòMath person‚Äô.\nStatQuest to quote host Josh Starmer, ‚ÄúStatQuest breaks down complicated Statistics and Machine Learning methods into small, bite-sized pieces that are easy to understand. StatQuest doesn‚Äôt dumb down the material, instead, it builds you up so that you are smarter and have a better understanding of Statistics and Machine Learning.‚Äù triple bam!\nThe Missing Semster of your CS Education - Fill in your missing skills needed for modern Data Science and Machine Learning, such as commandline usage, version control (Git)\nCalm code - Short and simple video lessons from starting scratch on using many open source tools, mainly in Python.\nKaggle - Kaggle is a great place to find datasets to practice on, even if you don‚Äôt want to enter comptetitions. You get free processing time in their (Jupyter-like) kernals, and you can learn by viewing solutions from other people. There are also numerous short courses to explore Data Visualisation, Feature Engineering, Geospatial analysis, Pandas, SQL, Time Series and many more.\nData School - Many short How-to videos (most under 5 minutes) covering Data Science & ML tools like SciKit-Learn, Pandas etc.\nR Shiny Tutorial - Turn your R into interactive analysis apps.\nü§ó Hugging Face - This course will teach you about natural language processing (NLP) using libraries from the Hugging Face ecosystem. Covers ü§ó Transformers, ü§ó Datasets, ü§ó Tokenizer, and ü§ó Accelerate ‚Äî as well as the Hugging Face Hub. It‚Äôs completely free and without ads.\nGetting Started with Object Detection using IceVision - IceVision is a Framework for object detection and deep learning that makes it easier to prepare data, train an object detection model, and use that model for inference. The IceVision Framework provides a layer across multiple deep learning engines, libraries, models, and data sets.\n28 Jupyter Tips & Tricks - These tips will increase your productivity in Jupyter, from key shortcuts to must-know plugins.\n\n\n\n\n\nBooks, Authors & Publishers\n\nGreen Tea Press - Allen Downey provides a collection of his books for free. Titles include Elements of Data Science, Think Stats, Think Bayes, Think Python and more. Many examples in Python with supporting code.\nAn Introduction to Statistical Learning - A foundational text that covers many methods for statistical learning with computers. Such as Linear Regression, Classification, Sampling, Tree-Based methods, Unsupervised Learning/Clustering & Deep Learning. Also a supporting course with videos\nHandbook of Regression Modeling in People Analytics Keith McNulty - Examples in R & Python. Free webversion, but book can also be purchased.\nPackt - Packt produce a wide variety of accessible books on technical topics. Like anything, the quality can vary by author. There is a free trial for 7 days to access all their books. And generally if you buy more than two books from Packt a year, consider getting a yearly subscription. - it is beter value, you can access all the books and also select some to keep in electronic form.\nManning MEAP - Manning MEAP program allows early access to books as they are being written and can purchase at a discount price. This is a an excellent way to learn new technologies and software, often before there are any mainstream resources available.\nJames D, McCaffrey - Blog. A researcher/developer at Microsoft and regular contributor of Machine Learning Columns to MSDN and Visual Studio Magazine. Many great ideas & snippets, across classical machine learning, statistics, and deep learning. Writes in C#, Javascript and Python. Like a box of chocolates..\nWeights & Biases - Articles - Weights & Biases or (wandb) is a fantastic service for tracking your machine learning project runs and to generate reports. (free for individuals). However, they are also one of the best educations sources for Machine Learning too. Weights & Biases - Videos\nR for Data Science - The starting point for learning R and Data Science. Co-authored by Hadley Wickham\n\n\n\n\nML News & Interviews\n\nML News. Regular news style updates. A simple way to keep up with current happenings in ML.\nMachine Learning Street Talk. In depth topic interviews.\nLex Fridman Interviews - Lex interviews the great minds of machine learning and artificial intelligence on his video podcast.\nTwo Minute Papers - Catch the latest papers in video, summmarised in two minutes.\nMicrosoft Research Podcast - Interviews with researchers in Microsoft in diverse research areas. What is their current focus area and the problems they are trying to solve?\n\n\n\n\nTools & Libraries\nThe choices have varied over the years, but currently falling into two camps. Tensorflow and Pytorch. In the end you will probably learn both, but currently more new papers are in Pytorch than Tensorflow. It is not too hard to move from one to the other.\n\nTensorFlow - Much improved ease of use since V2 and using Keras as the primary API. If you want to run ML in the browser via javascript. Tensorflow is the better option over Pytorch with tensorflow.js. Keras API can be used with multiple backends, not just Tensorflow, so there may be future systems that will use it.\nPytorch - Whilst Google was still inflicting dreadful programming interfaces with Tensorflow V1, much of the research world moved to Pytorch. It is flexible and expandible and well supported with blogs and books.\nFastAI - Fastai library extends Pytorch with a layed API, providing SOTA defaults and best practice training loop and increased productivity.\nML.Net - ML.Net provides a AutoML tools and ML pipeline for use on dotnet platforms. It has promise, but documentation & examples are limited and often broken.\nFLAX Flax is a high-performance neural network library and ecosystem for JAX that is designed for flexibility. The new cool kid on the block.\nPytorch Image Models (timm) - timm is a deep-learning library created by Ross Wightman and is a collection of SOTA computer vision models, layers, utilities, optimizers, schedulers, data-loaders, augmentations and also training/validating scripts with ability to reproduce ImageNet training results.\nHugging Face Spaces - Share your (Python) ML model applications in a few minutes. Spaces are a simple way to host ML demo apps directly on your profile or your organization‚Äôs profile. This allows you to create your ML portfolio, showcase your projects at conferences or to stakeholders, and work collaboratively with other people in the ML ecosystem.\nShiny - Shiny is an R package that makes it easy to build interactive web apps straight from R. You can host standalone apps on a webpage or embed them in R Markdown documents or build dashboards. You can also extend your Shiny apps with CSS themes, htmlwidgets, and JavaScript actions.\nTorchSharp - TorchSharp is a wrapper .NET library that provides access to the library that powers PyTorch. Allowing C# and F# to more easily use Pytorch from those languages on Windows & Linux. It is part of the .NET Foundation. Examples repo.\n\n\n\n\nPapers\nFinding and reading papers is a key skill to develop particularly in Deep Learning, where you may have to implement the technique yourself because it may not make it into your favourite tools for a while.\n\nPapers With code - I‚Äôm a firm believer that results in papers should be replicable. Papers with code is an excellent resource for machine learning papers that have provided code and also shows the SOTA (state of the art) results for papers and related benchmark datasets.\n\nSemantic Scholar - An excellent free AI-powered research tool for scientific literature. Results includes citations (and the egotistical H-Index)\nW&B Paper Reading Group - Aman Arora hosts this Deep Learning Paper reading group for beginners. Step by step Video walk-throughs of key papers covering ML architectures such as Resnets, DETR, Squeeze & Excitation Nets etc. A great place to begin learning through reading papers.\nTwo Minute Papers Catch the latest papers in video, summmarised in two minutes.\nNLP Progress Tracks the state of the art (SOTA) for numerous Natural Language Processing tasks.\nTrending Papers Displays trending ML papers, recent, weekly or monthly feed-view. Also has search.\nAnnotated Papers A collection of simple PyTorch implementations of neural networks and related algorithms. These implementations are documented with explanations, and the website renders these as side-by-side formatted notes. We believe these would help you understand these algorithms better.\n\n\n\n\nBlogging Tools\n\nJekyll Cheatsheet - Options for use with Jekyll static sites. This cheat sheet serves as a quick reference of everything Jekyll can do.\nFastPages - Deprecated. Use Quarto instead. FastPages to Quarto Migration Guide FastPages: Turn your Jupyter Notebooks, Word and Markdown documents into Blog posts. fastpages automates the process of creating blog posts via GitHub Actions, so you don‚Äôt have to fuss with conversion scripts.\nQuarto - Quarto¬Æ is an open-source scientific and technical publishing system built on Pandoc\n\nCreate dynamic content with Python, R, Julia, and Observable.\nAuthor documents as plain text markdown or Jupyter notebooks.\nPublish high-quality articles, reports, presentations, websites, blogs, and books in HTML, PDF, MS Word, ePub, and more.\nAuthor with scientific markdown, including equations, citations, crossrefs, figure panels, callouts, advanced layout, and more.\n\nMarkdown Guide for Jupyter - A guide to the Markdown format, so you can get the most from your Jupyter Notebook writing."
  },
  {
    "objectID": "posts/2021-10-25-is-the-machine-learning.html",
    "href": "posts/2021-10-25-is-the-machine-learning.html",
    "title": "Is the machine learning?",
    "section": "",
    "text": "That moment for me was in the computer room at my high school in around 1980. At the time computers were not cool, and certainly those of us in the computer club were not cool, pocket protectors were cool though right? Then I saw a program on a ‚Äòpersonal computer‚Äô that blew my mind, the computer learned by asking questions!\n\nThe lead up.\nIn the early ‚Äò80s most of our ‚Äôcomputer‚Äô lessons were in Math classes, after having learnt a few rudimentaries of branching and looping, we would write out our program in our notebooks. Then painstakingly transcribe these onto optical-mark cards before the end of the lesson, trying very hard to avoid mistakes.\n\n\n\noptical-card.jpg\n\n\nThese punchcards were bundled up and sent to the local university to be run. The following week (if we were lucky) we would get our program output back, printed on multi-fold paper. It was a long time to wait, just to find you‚Äôd made a simple syntax error or the program was halted to prevent an infinite loop. The long ‚Äòdebug‚Äô cycle taught us to be very, very careful.\n\nMany students gave up at this point.\n\nThere was no encouragement in that long ‚Äòdebug‚Äô cycle to do anything interesting, nothing beyond replication of calculators. User interaction wasn‚Äôt a thing in such a process. A later staple of introductory programming classes, guessing .‚Äùhigher or lower‚Äù would have taken months of round trips. It is why I chuckle to myself when someone complains that the internet is slow.\n\n\nThe changes.\nIt was around this time, with the help of the Math department that we began raising money for a computer for the school. And luckily it was also around this time that the government injected funds into the school curriculum specifically to help schools purchase computers. The computer club became a hive of activity, and before I left high school we had many computers. 5 to be exact. From memory, there was an Apple IIe, Ohio Scientific Challenger 1P 4K-Ram, two Ohio Challenger 4p 8K-Ram and a Commodore Vic 20 (it had colours-wow!!). To me the Ohio‚Äôs were beautiful wooden sided, metal cased machines - we learnt to program from the BASIC-IN-ROM 16 page manual. (You can play with an Emulator here).\n\n\n\n2009-04-26-challenger fixed.jpg\n\n\nWithout these computers the moment may never have happened.\n\nI feel like Grandpa Simpson when I talk about these things, and I am grateful at technical meetups for the few younger people who are genuinely interested.\n\n\n\nThe moment.\nThe day that opened up my lifelong fascination with computers, programming and machine learning was when one of my classmates implemented a program version of the guessing game. ‚ÄúWhat animal am I thinking of?‚Äù.\nA game where you try to guess what animal the person is thinking of, by asking a series of questions that can be answered ‚Äúyes‚Äù or ‚Äúno‚Äù, until you feel you have enough information to guess. It usually it follows a sequence such as.\n- Does it have a 4 legs?\n    - No\n- Does it have feathers?\n    - Yes\n- Does it swim?\n    - Yes\n- Is it a duck? \n    - Yes. You guessed correctly!\nI‚Äôd seen guessing games before, usually in the form of number guessing ‚Äúhigher / lower‚Äù, or just guessing from a predifined list.\nBut what set this particular program apart from anything else I had seen upto that point was that by seemingly magic, is that could learn!. If it could not guess the answer correctly it would ask the user for information to help it answer correctly in the future. My tiny teen mind was blown. I had no idea at the time how it worked, but I took it as challenge to find out.\nIn context you have to remember, there was no hard-drive, floppy disk, no network, and barely enough memory to hold the program.\nWe had simple 20 questions type programs where we had pre-programmed all of the options and knew the outcome expected at any point. This was the first time I had seen a program know more than the original programmer had told it. This was the turning point, the big moment where I knew programming and computers was for me. It didn‚Äôt matter that I wasn‚Äôt allowed to go to university to study computing because I had studied humanities instead of physics or that the Australian Computer Society at the time wanted to prevent anyone without a computer degree from getting a role with computers. Nothing would stop me.\nThankfully society has moved on, the internet and open source communities have broken down those wall, there are many free resources and opportunities today.\n\n\nWhat animal am I thinking of now?\nRecently whilst working through the book Conceptual Programming with Python, one of the examples was a version of the program. And here it is in Python using classes, largely as per that book. Thank you to the authors Thorsten Altenkirch & Isaac Triguero for bringing back some happy memories.\n(I had thought about re-writing the code to work in the emulator, I may do that in future. If you take up that challenge, please let me know.)\n\n#slow\n# from Conceptual Programming with Python, with minor modification.\nclass Knowledge:\n        pass\n\nclass Answer(Knowledge): \n    def __init__(self,text):\n        self.text = text\n\n    def play(self):\n        if ask(\"Were you thinking of a {}?\".format(self.text)):\n            print(\"I knew it!\")\n        else:\n            newanimal = input(\"What animal were you thinking of?\")\n            newquestion = input(\"What is a question to distinguish between {} and {}?\".format(self.text,newanimal))\n            if ask(\"for {}, what should the answer be (y/n)?\".format(newanimal)): \n                return Question(newquestion, Answer(newanimal),self)\n            else:\n                 return Question(newquestion, self, Answer(newanimal))\n\nclass Question(Knowledge):\n    def __init__(self, text, ifyes, ifno):\n        self.text, self.ifyes, self.ifno = text, ifyes, ifno\n    \n    def play(self):\n        if ask(self.text):\n            self.ifyes = self.ifyes.play()\n        else:\n            self.ifno = self.ifno.play()\n        return self\n\ndef ask(q):\n    while True:\n        ans = input(q+\" \")\n        if ans==\"y\":\n            return True\n        elif ans==\"n\":\n            return False\n        else:\n            print(\"Please answer y or n!\")    \n\n#------------------------\n\nimport pickle\ntry : \n    file = open(\"animal.kb\",\"rb\")\n    kb = pickle.load(file)\n    file.close()\nexcept FileNotFoundError:\n    # seed with a few defaults\n    kb = Question(\n    \"Does it have 4 legs?\",\n    Question(\"Does it bark?\", \n             Answer(\"dog\"), Answer(\"cat\")),Answer(\"bird\"))             \n\nwhile True:\n    print(\"I can guess what animal you are thinking of.\" )\n    if not ask(\"Do you want to play?\"):\n        break\n    kb = kb.play()    \n\n    if not ask(\"Do you want to save changes?\"):\n        break\n    file = open(\"animal.kb\",\"wb\")\n    pickle.dump(kb, file)\n    file.close\n   \n\nI can guess what animal you are thinking of.\nDo you want to play?  y\nDoes it have 4 legs?  y\nDoes it bark?  n\nWere you thinking of a cat!  n\nWhat animal were you thinking of? rabbit\nWhat is a question to distinguish between cat and rabbit? Does it eat carrots?\nfor rabbit, what should the answer be (y/n)?  y\nDo you want to save changes?  y\n\n## And on the next round, it knows what a rabbit is..\n\nI can guess what animal you are thinking of.\nDo you want to play?  y\nDoes it have 4 legs?  y\nDoes it bark?  n\nDoes it eat carrots?  y\nWere you thinking of a rabbit?  y\nI knew it!\n\n\nThe Verdict\nYes, by modern standards it is pretty lame. But it was fun and inspiring then. I‚Äôm sure the modern ML achievements we have to today will seem as equally laughable to someone 40 years from now.\nWas the machine learning? It certainly would not pass a Turing test, but it did behave as science fiction books lead me to expect. So teenage me will say ‚ÄúYes!, the machine was learning‚Äù.\nThe door to the future had opened, there was no turning back. I‚Äôve been hooked on programming and machine learning ever since."
  },
  {
    "objectID": "posts/2021-11-08-snippets.html",
    "href": "posts/2021-11-08-snippets.html",
    "title": "Snippets, Notes and Reminders.",
    "section": "",
    "text": "Fastai show_install()\nWhen working with fastai. This emits a summary of the install fastai, pytorch and cuda libraries and paths. Usefull for reporting issues and for finding the differences in between dev envs.\nfrom fastai.test_utils import show_install\nshow_install()\n=== Software === \npython        : 3.7.12\nfastai        : 2.5.3\nfastcore      : 1.3.27\nfastprogress  : 0.2.7\ntorch         : 1.9.0+cu111\nnvidia driver : 460.32\ntorch cuda    : 11.1 / is available\ntorch cudnn   : 8005 / is enabled\n\n=== Hardware === \nnvidia gpus   : 1\ntorch devices : 1\n  - gpu0      : Tesla K80\n\n=== Environment === \nplatform      : Linux-5.4.104+-x86_64-with-Ubuntu-18.04-bionic\ndistro        : #1 SMP Sat Jun 5 09:50:34 PDT 2021\nconda env     : Unknown\npython        : /usr/bin/python3\nsys.path      : \n/content\n/env/python\n/usr/lib/python37.zip\n/usr/lib/python3.7\n/usr/lib/python3.7/lib-dynload\n/usr/local/lib/python3.7/dist-packages\n/usr/lib/python3/dist-packages\n/usr/local/lib/python3.7/dist-packages/IPython/extensions\n/root/.ipython\n\n\n\nAliases\nAliases save typing for common commands and make it easier to remember less common ones. Use naming pnemonics that make sense to you, customise to your workflow/memory. After saving, open a new bash terminal (so the changes can be loaded) and you should be able to run your new ‚Äòcommands‚Äô. You can also see the commands turn up in auto complete (tab tab). Be careful not to unintentionally name an alias the same as another utility.\nIn a standard install of ubuntu (via wsl(https://ubuntu.com/wsl) in my case), you will find there may be some aliases in ~/.bashrc but I prefer to separate them into ~/.bash_aliases. ~/.bashrc looks for this alias file when it loads. if ~/.bash_aliases doesn‚Äôt exist, you can create it.\n# editxxx files\nalias editalias='nano ~/.bash_aliases'\nalias editbash='nano ~/.bashrc'\n\n# cdxxx change to dir\nalias cdproj='cd /mnt/d/proj'\n\n#jupyter lab\nalias jl='jupyter-lab --no-browser'\n\n# goxxx use for environments\nalias godev='conda activate fastaidev'\nalias gotest='conda activate fastaitest39'\n\n#python utils\nalias showsys='python -c \"import sys, pprint; pprint.pprint(sys.path)\"'\nalias showfast='python -c \"from fastai.test_utils import show_install; show_install()\"'\nalias showcuda='python -c \"import torch; print(torch.cuda.is_available())\"'\n\n\n\n\nGit Basics\nJust the basics. A more extensive fastai git reference here\nClone Repo\ngit clone <repo-url>\nBasic configuration\ngit config --global user.name <your name>\ngit config --global user.email <your email>\nInitialise a repository\ngit init\nView status\ngit status\nView changes (diffs)\n# changes to file\ngit diff <path to file>\n\n# compare files - see difference\ngit diff <git id 1>..<git id 2>\nView commit history\n# View all commits. (most recent are shown first)\ngit log \n\n# View last n commits (e.g last 5).\ngit log -5\n\n\nGit Commits\nStage changes\ngit add <path to file>\nCommit changes\n# commit staged files\ngit commit -m \"<your message>\"\n\n# commit specified file\ngit commit <path to file> -m \"<your message>\"\n\n# commit all changed files\ngit commit -am \"<your message>\"\nReturn to a previous version\ngit checkout <git id> <path to file>\nPush up changes\ngit push\nPull down changes\ngit pull\n\nGit Branching\nNew branch\ngit branch <branch-name>\nSwitch branch\ngit checkout <branch-name>\nMerge branch with current branch\ngit merge <branch-name>\nCheckout branch - (switch to branch)\ngit checkout <branch-name>   \nSet up branch on remote (GitHub etc)\ngit push --set-upstream origin <branch-name>\n\n\n\n\nCreating Fastai PR\nFrom FastAI - Dev Setup\nCreate new branch and check it out.\ngit checkout -b <pr-branch-name>\nMake changes & test. Remember to run nbdev_clean before\ngit commit -am \"<commit message>\"\nThe first time you push from your fork, you need to add -u origin HEAD, but after the first time, you can just use git push\ngit push -u origin HEAD\nCreate the PR. To use the information from your commit message as the PR title, just run\ngh pr create -f\nTo be interactively prompted for more information (including opening your editor to let you fill in a detailed description), just run gh pr create without the -f flag. As you see above, after it‚Äôs done, it prints the URL of your new PR - congratulations, and thank you for your contribution!\n\n\nPost Fastai PR steps\nTo keep your fork up to date with the changes to the main fastai repo, and to change from your pr branch back to master, run:\ngit pull upstream master\ngit checkout master\nIn the future, once your PR has been merged or rejected, you can delete your branch if you don‚Äôt need it any more:\ngit branch -d test-pr\n\n\n\nAdding Timing to Jupyter Lab/Notebooks Cells.\nDisplay execute time and last run to your jupyter cells with this timer extension.\nJupyterLab: https://github.com/deshaw/jupyterlab-execute-time\nI have only used/tested the Jupter Lab version. I assume the Jupyter Notebook version works similarly.\n\n\n\njupyter-timer-snapshot.png\n\n\n\nRequirements\n\nJupyterLab >= 3.0\n\n\n\nInstall\nTo install this package with pip run\npip install jupyterlab_execute_time\nTo install this package with conda run\nconda install -c conda-forge jupyterlab_execute_time\nNote: for this to show anything, you need to enable cell timing in the notebook via Settings->Advanced Settings Editor->Notebook: {\"recordTiming\": true}.\nThis is a notebook metadata setting and not a plugin setting. The plugin just displays this data.\n\n\n\njupyter-timer-settings.png\n\n\nOnce installed and appropriate setting has been added, you may be to restart the kernel (or jupyter lab) before you see the results.\n\n\n\nBuild a Jupyter Document from a json string\nExample from Issac.Flath #fastai-help discord. How to structure the output for a Jupyter doc. A jupyter doc is only a specially formated json file\nCan also use an API e.g https://nbformat.readthedocs.io/en/latest/api.html#module-nbformat.v4\ncode1 = \"\"\"\nimport pandas as pd\nimport numpy as np\n\"\"\"\ncode2 = \"def some_function(): return 'a silly string'\"\n \nimport json\n# this is the notebook structure\nnb = {\"cells\": [],\"metadata\": {},\"nbformat\": 4, \"nbformat_minor\": 4}\n\ndef write_code_cell(code): return {\"cell_type\": \"code\",\"execution_count\": 0,\"metadata\": {},\"outputs\": [],\"source\": [code]}\n\nfor code in [code1,code2]: nb[\"cells\"].append(write_code_cell(code))\n\nwith open('output.ipynb', 'w') as f: f.write(json.dumps(nb))"
  },
  {
    "objectID": "posts/ddpm-kamon-01.html",
    "href": "posts/ddpm-kamon-01.html",
    "title": "Generating Kamon Designs",
    "section": "",
    "text": "Last week I began the new ‚ÄòFrom Deep Learning Foundations to Stable Diffusion‚Äô course , which is part 2 of FastAi‚Äôs Practical Deep Learning for Coders series. The first lesson kicked off quickly, getting into the sizzle of Stable Diffusion, the phenomenal model which empowers people to generate images of fantastic beauty on moderately powered home computers.\nI decided to train my own Kamon generator. I completed building a training set that I had gathered last year for a GAN project. The Kamon Dataset can be found here.\n\n\n\nmon-white-sample.png\n\n\nUsing this Diffusion_models_with_fastai notebook from Tanishq Abraham, with a slight modification to use my Kamon dataset (instead of mnist) and adjusting a few settings to fit on my 3090 card.\nbs = 16 # batch size\nsize = 224 # image size\nI also found it was necessary to skip the learning rate finder. ddpm_learner.lr_find() as that process would always run out of cuda memory. For training, I used a learner rate of ‚Äò3e-4‚Äô and pushed fit_one_cycle to 100 epochs.\nddpm_learner.fit_one_cycle(100, 3e-4)\nOn average, training this model took 2.5 minutes per epoch. (Far better than days spent last year trying to do similar with Stylegan2 only to end in model collapse.)\n\n\n\n\n\nIt has managed some nice clean lines, boundary areas and a few motifs, and very little noise. As I learn more I will revisit this project and improve it.\nA few batches of generated images. \n\n\n\nkamon-ddpm-02.png\n\n\n\n\n\nkamon-ddpm-03.png"
  }
]